{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61376dc9",
   "metadata": {},
   "source": [
    "# COMP 551 — Assignment 4\n",
    "\n",
    "Authors:\n",
    " - Bernier, Audréanne\n",
    " - Coull-Neveu, Ben\n",
    " - Trachsel-Bourbeau, Anjara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e00c5",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b52a833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import unicodedata\n",
    "\n",
    "import os\n",
    "import pickle # to save models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5f7e9",
   "metadata": {},
   "source": [
    "Plot defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf40374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['legend.fontsize'] = 'medium'\n",
    "plt.rcParams['legend.fancybox'] = False\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['grid.linewidth'] = 0.5\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "plt.rcParams['axes.autolimit_mode'] = 'data'  # default, ensures autoscale uses data\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "\n",
    "# set default save directory and parameters\n",
    "SAVEDIR = './figures/'\n",
    "MODELDIR = './models/'\n",
    "os.makedirs(SAVEDIR, exist_ok=True)\n",
    "os.makedirs(MODELDIR, exist_ok=True)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182ceb1",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6dc1b",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "X is input data that include text sequences\n",
    "\n",
    "Y is target value \n",
    "\n",
    "YL1 is target value of level one (parent label) --> scientific field {0-6}\n",
    "\n",
    "YL2 is target value of level one (child label) --> subfield {0-32}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d78c0e",
   "metadata": {},
   "source": [
    "Labels in YL1 (scientific field):\n",
    "\n",
    "- 0 - Computer Science\n",
    "- 1 - Electrical Engineering\n",
    "- 2 - Psychology\n",
    "- 3 - Mechanical Engineering\n",
    "- 4 - Civil Engineering\n",
    "- 5 - Medical Science\n",
    "- 6 - Biochemistry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc0be83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, dtype=str)->np.ndarray:\n",
    "    # read file \n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = [line.strip() for line in f]  # list of all lines\n",
    "\n",
    "    if dtype == int:\n",
    "        return [int(x) for x in lines]  # make sure target lists are int type\n",
    "    \n",
    "    return np.array(lines)  # return array\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "\n",
    "    text = unicodedata.normalize(\"NFKD\", text) # droping accents\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "    punctuation = \",.;:!?(){}[]\\\"'*/\\\\\" # removing punctuation\n",
    "    for p in punctuation:\n",
    "        text = text.replace(p, \" \")\n",
    "\n",
    "    cleaned = []\n",
    "    for ch in text:\n",
    "        if ch.isalnum() or ch == '-' or ch == ' ': # keeping only alphanumeric, hyphens and spaces\n",
    "            cleaned.append(ch)\n",
    "        else:\n",
    "            cleaned.append(\" \")\n",
    "    text = \"\".join(cleaned)\n",
    "\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \") # remove double spaces\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "class WOSDataset(Dataset):\n",
    "    # class to build a torch dataset from some txt files\n",
    "    # IF WE DO TRAIN/TEST SPLIT ADD IT IN HERE AND RETURN TRAIN DATASTE & TEST DATASET SEPARATELY\n",
    "\n",
    "    def __init__(self, dataset_dir):\n",
    "        self.X = load_data(dataset_dir + 'X.txt')\n",
    "        self.Y = load_data(dataset_dir + 'Y.txt', dtype=int)\n",
    "        self.YL1 = load_data(dataset_dir + 'YL1.txt', dtype=int)\n",
    "        self.YL2 = load_data(dataset_dir + 'YL2.txt', dtype=int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw = self.X[idx]\n",
    "        cleaned = clean_text(raw)\n",
    "        return raw, self.Y[idx], self.YL1[idx], self.YL2[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff9e9744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 11967\n",
      "Number of classes in YL1 (parent labels): 7\n",
      "Number of classes in YL2 (child labels): 5\n",
      "32\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' appear to be disrespectful and suggestions of how this can be avoided are made.',\n",
       " 'r consumption and in the total cost. (C) 2015 Elsevier Ltd. All rights reserved.',\n",
       " 'ons about human capital quality, information technology and information sharing.',\n",
       " 'e of them are generalizations of known ones. (C) 2016 Published by Elsevier Inc.',\n",
       " 't age possible for children with LI. (C) 2017 Elsevier Ltd. All rights reserved.',\n",
       " 'd indicators of control, and offer reference for the construction of the tunnel.',\n",
       " 'ing high-efficiency 4G/5G/WLAN broadband wireless silicon PAs will be discussed.',\n",
       " ' is intact in ASC, but is not socially modulated by gaze, as predicted by STORM.',\n",
       " 'terdependent situations; and carrying out effective instead of bungling actions.',\n",
       " ' ProteomeXchange identifier PXD004078 and GEO Series accession number GSE82146).',\n",
       " ' of the machine. (C) 2016 IAgrE. Published by Elsevier Ltd. All rights reserved.',\n",
       " ' care and other healthcare professionals at a national, local or practice level.',\n",
       " 'segmentation methods in all tested datasets. (C) 2016 Published by Elsevier Inc.',\n",
       " 'een landscaping engineering companies when make scientific procurement decision.',\n",
       " 'an efficient and cost saving manner. (C) 2017 Elsevier B.V. All rights reserved.',\n",
       " 'ics, i.e., controlled flow, underbalanced drilling, and foam cementing, as well.',\n",
       " 'er modulation of this receptor can serve as a novel therapeutic strategy for AD.',\n",
       " 'mware, the acquisition techniques and the performances of the new RF controller.',\n",
       " 'harvesting system will be developed. (C) 2014 Elsevier Ltd. All rights reserved.',\n",
       " 'te the importance of hand hygiene and clean operation for aseptic manufacturing.',\n",
       " 'idates proposed by both approaches is provided and their obtention is discussed.',\n",
       " 'nges in characterizing group differences, particularly between MCI and controls.',\n",
       " 'd energy resource controllers, laboratory battery testers, and welding machines.',\n",
       " 'osystem services, and human well-being and to achieve a more sustainable future.',\n",
       " 'ations, showed stable improvement of the appearance-based object classification.',\n",
       " ' explanation for observed behaviour. (C) 2014 Elsevier Ltd. All rights reserved.',\n",
       " ', we discuss these topics and relate them to the future directions of the field.',\n",
       " 't also denotes older adults can analogize between realistic faces and emoticons.',\n",
       " ' clinical management of this disease. (C) 2016 Elsevier Inc All rights reserved.',\n",
       " 'lved in soil and water conservation. (C) 2017 Elsevier Ltd. All rights reserved.',\n",
       " 'on canal, then the harvest should be at the point of the minimum net production.',\n",
       " 'ant gains in terms of the secrecy rate over schemes designed for Gaussian input.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading WOS11967\n",
    "dataset_dir = './datasets/WOS11967/'\n",
    "WOS11967_dataset = WOSDataset(dataset_dir)\n",
    "\n",
    "# Get dataloaders\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(WOS11967_dataset, batch_size=32, shuffle=True)\n",
    "# Note: X are strings so they currently get returned as a tuple when we get a batch\n",
    "# eg. sample_X = (\n",
    "#     \"abstract1\",\n",
    "#     \"abstract2\",\n",
    "#     \"abstract3\",\n",
    "#     ...\n",
    "# )\n",
    "\n",
    "\n",
    "# Check data\n",
    "num_instances = len(WOS11967_dataset)\n",
    "print(f'Total number of samples: {len(WOS11967_dataset)}')\n",
    "\n",
    "num_classes_YL1 = len(set(WOS11967_dataset.YL1))\n",
    "print(f'Number of classes in YL1 (parent labels): {num_classes_YL1}')\n",
    "\n",
    "num_classes_YL2 = len(set(WOS11967_dataset.YL2))\n",
    "print(f'Number of classes in YL2 (child labels): {num_classes_YL2}')\n",
    "\n",
    "\n",
    "# Check a sample (one batch)\n",
    "sample_X, sample_Y, sample_YL1, sample_YL2 = next(iter(data_loader))\n",
    "print(len(sample_X))  # (batch_size,)  THIS IS A TUPLE OF STRINGS\n",
    "print(sample_Y.shape)  # (batch_size,)\n",
    "last_30_all = [x[-80:] for x in sample_X]\n",
    "last_30_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79cd43",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Things to clean (done):\n",
    "- special charaters (accents, non-ASCII characters, etc)\n",
    "- capitalization\n",
    "- Remove tabs or non-single spaced spaces\n",
    "- Remove poncutation, eg: , . ; : ( ) [ ]\n",
    "\n",
    "Things to clean (to do):\n",
    "- some abstracts end in citations, eg: (C) 2016 Elsevier B.V. All rights reserved.\n",
    "- some abstracts contain weblinks, eg: the Menpo Project (http://www.menpo.org)\n",
    "- some have institution of origin mentioned, eg: School of Aerospace, Mechanical and Manufacturing Engineering, RMIT University\n",
    "- some use periods as part of chemical/physical formulas: eg, reduce PM2.5 effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-procesing data for LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing data for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878078d",
   "metadata": {},
   "source": [
    "# Implement LSTM class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15122a",
   "metadata": {},
   "source": [
    "- can use pytorch.nn.Module\n",
    "- For the LSTM model, you must design the data pre-processing pipeline that turns the unstructured text data into\n",
    "numerical features. You are free to choose your encoding method, including pre-trained methods like word2vec;\n",
    "however, there should be some justification for your choice in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e4849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y):  # MAKE THIS ACCEPT A TORCH DATASET SO WE CAN USE THE GPU\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):  # SAME FOR HEREEEE!!\n",
    "        pass\n",
    "    \n",
    "    def evaluate_acc(self, Y, Yhat):\n",
    "        return np.mean(Y == Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d4ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d55c16",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba7db7",
   "metadata": {},
   "source": [
    "- For BERT, you can use the transformers package to tokenize the input text and convert the tokens into numeri-\n",
    "cal features https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed5286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc288f44",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae600c0",
   "metadata": {},
   "source": [
    "## some experiment etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a60fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp551",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
