{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700e45bc",
   "metadata": {},
   "source": [
    "# COMP 551 — Assignment 3\n",
    "\n",
    "Authors:\n",
    " - Bernier, Audréanne\n",
    " - Coull-Neveu, Ben\n",
    " - Trachsel-Bourbeau, Anjara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda2794",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e62a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f5b6d",
   "metadata": {},
   "source": [
    "CHECKLIST\n",
    "- Implementation of data normalization (1 points)\n",
    "- Implementation and training of MLP with no hidden layers (1 points)\n",
    "- Implementation and training of MLP with one hidden layers and ReLU activation (3 points)\n",
    "- Implementation and training of MLP with two hidden layers and ReLU activation (1 points)\n",
    "- Implementation and training of the two hidden layers MLP with tanh activation (2 points)\n",
    "- Implementation and training of the two hidden layers MLP with Leaky-ReLU activation (2 points)\n",
    "- L1 regularization (2 points)\n",
    "- L2 regularization (2 points)\n",
    "- Train MLP without normalization (1 points)\n",
    "- Correctly plot and compare the results of the 8 trained models above (5 points)\n",
    "- Train the MLP with the 128 x 128 FashionMNIST data (1 points)\n",
    "- Plot the results of the larger models, and compare classification performance and training time (2 points)\n",
    "- Implement and train the CNN (2 points)\n",
    "- Re-train the CNN with the 128 x 128 FashionMNIST data (1 points)\n",
    "- Plot the results of the two CNN trained, and compare classification performance and training time to the best MLP. (4 points)\n",
    "- Implement and train the pre-trained model with the trainable fully connected layer(s). (3 points)\n",
    "- Plot the results of the pre-trained model, and compare its performance to the best MLP and regular CNN.(4 points)\n",
    "- Run an experiment to justify the choice of fully connected layers for the pre-trained model, and show\n",
    "supporting plots. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b180a",
   "metadata": {},
   "source": [
    "You can report your findings either in the form of a table or a plot in the write-up. However, include in your\n",
    "colab notebooks the plots of the test and train performance of the MLPs / CNN / pre-trained model as a function\n",
    "of training epochs. This will allow you to see how much the network should be trained before it starts to overfit\n",
    "to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d972e1a",
   "metadata": {},
   "source": [
    "Note 2: We expect you to provide plots/tables in your report that justifies your choice of hyperparameters\n",
    "(the learning rates of the MLPs / CNNs / pretrained models, the architectural parameters of the CNNs and\n",
    "pretrained models). You are not required to perform cross-validation in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371ee76",
   "metadata": {},
   "source": [
    "# Implement MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ac04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))  # softmax\n",
    "relu = lambda z: np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66478ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Supports L=0,1,2 hidden layers.\n",
    "\n",
    "    Constructor takes:\n",
    "    g = output activation function\n",
    "    h = hidden activation function\n",
    "    L = number of hidden layers\n",
    "    M = number of hidden units, iterable (each element corresponds to a layer)\n",
    "    \n",
    "    -> Weights and biases are initialized randomly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g=None, h=None, L=1, M=64, D=None): \n",
    "        self.g = g if g is not None else lambda x: x  # default identity\n",
    "        self.h = h if h is not None else lambda x: x\n",
    "        self.L = L\n",
    "        \n",
    "        if isinstance(M, int):\n",
    "            self.M = [M] * L  # same num of units in each layer\n",
    "        else:\n",
    "            assert len(M) == L, \"Length of M must equal L\"\n",
    "            self.M = M\n",
    "        \n",
    "        # Initialize weights & biases depending on # of layers (L)\n",
    "        assert D is not None, \"Need number of features\"\n",
    "        if L == 0:  # no hidden layer\n",
    "            self.w = np.random.randn(D) * 0.01\n",
    "            self.b = 0.0\n",
    "        elif L == 1:  # 1 hidden layer\n",
    "            self.v = np.random.randn(D, self.M[0]) * 0.01  # 1st hidden layer, D x M\n",
    "            self.c = np.zeros(self.M[0])\n",
    "            self.w = np.random.randn(self.M[0]) * 0.01  # output layer, M x 1\n",
    "            self.b = 0.0\n",
    "        elif L == 2:  # 2 hidden layers\n",
    "            self.v1 = np.random.randn(D, self.M[0]) * 0.01  # 1st hidden layer, D x M1\n",
    "            self.c1 = np.zeros(self.M[0])\n",
    "            self.v2 = np.random.randn(self.M[0], self.M[1]) * 0.01  # 2nd hidden layer, M1 x M2\n",
    "            self.c2 = np.zeros(self.M[1])\n",
    "            self.w = np.random.randn(self.M[1]) * 0.01  # output layer, M2 x 1\n",
    "            self.b = 0.0\n",
    "\n",
    "    \n",
    "    # NEED TO FIX BELOW TO SUPPORT L=0,1,2 AND THE DIFFERENT ACTIVATIONS\n",
    "    # IMPLEMENT BACK PROP MUNALLY FOR ALL CASES (SEE ED ANNOUNCEMENT #384)\n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            v, w = params\n",
    "            z = logistic(np.dot(x, v)) #N x M\n",
    "            yh = logistic(np.dot(z, w)) #N\n",
    "            dy = yh - y #N\n",
    "            dw = np.dot(z.T, dy)/N #M\n",
    "            dz = np.outer(dy, w) #N x M\n",
    "            dv = np.dot(x.T, dz * z * (1 - z))/N #D x M\n",
    "            dparams = [dv, dw]\n",
    "            return dparams\n",
    "        \n",
    "        w = np.random.randn(self.M) * .01\n",
    "        v = np.random.randn(D,self.M) * .01\n",
    "        params0 = [v,w]\n",
    "        self.params = optimizer.run(gradient, x, y, params0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        v, w = self.params\n",
    "        z = logistic(np.dot(x, v)) #N x M\n",
    "        yh = logistic(np.dot(z, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e002be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, params):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            grad = gradient_fn(x, y, params)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb17bb",
   "metadata": {},
   "source": [
    "# Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a6393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d4ca92",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd2e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860) tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "# Compute mean and std from train dataset (for normalization)\n",
    "dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "data = dataset.data.float() / 255.0  # convert to float and scale to [0,1]\n",
    "\n",
    "# Compute mean and std over all training set (since greyscale images)\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca989841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FashionMNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), # scales to [0,1]\n",
    "                                transforms.Normalize((mean,), (std,))  # mean 0, std 1\n",
    "                                ])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    "    )\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e86ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108ade3",
   "metadata": {},
   "source": [
    "\"In the context of images, normalization means converting pixel values (originally in the range [0, 255]) to a specific range, usually between 0 and 1 or -1 and 1, usually by first scaling to [0, 1] (i.e., dividing by 255), and then subtracting the mean and dividing by the standard deviation, either per-channel (for color images) or over the entire training set (for grayscale). This helps stabilize training by centering the data and ensuring all features (pixels) contribute proportionally during optimization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37ad6e",
   "metadata": {},
   "source": [
    "# MLP Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95173907",
   "metadata": {},
   "source": [
    "## 1 - Vary number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e23c2",
   "metadata": {},
   "source": [
    "First of all, create three different models: (1) an MLP with no hidden layers, i.e., it directly maps the inputs\n",
    "to outputs, (2) an MLP with a single hidden layer having 256 units and ReLU activations, (3) an MLP with 2\n",
    "hidden layers each having 256 units with ReLU activations. It should be noted that since we want to perform\n",
    "classification, all of these models should have a softmax layer at the end. After training, compare the test\n",
    "accuracy of these three models on the FashionMNIST dataset. Comment on how non-linearity and network\n",
    "depth affects the accuracy. Are the results that you obtain expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0 Hidden Layers ---\n",
    "model0 = MLP(g=logistic, L=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1 Hidden Layer ---\n",
    "model1 = MLP(g=logistic, h=relu, L=1, M=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2 Hidden Layers ---\n",
    "model2 = MLP(g=logistic, h=relu, L=2, M=[256, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bf384",
   "metadata": {},
   "source": [
    "## 2 - Changing activations in 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea746c",
   "metadata": {},
   "source": [
    "Take the last model above, the one with 2 hidden layers, and create two different copies of it in which the\n",
    "activations are now tanh and Leaky-ReLU. After training these two models compare their test accuracies with\n",
    "model having ReLU activations. Comment on the performances of these models: which one is better and why?\n",
    "Are certain activations better than others? If the results are not as you expected, what could be the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77be109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccbd252a",
   "metadata": {},
   "source": [
    "## 3 - Adding regularization to 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf6a13",
   "metadata": {},
   "source": [
    "Create an MLP with 2 hidden layers each having 256 units with ReLU activations as above. However, this\n",
    "time, independently add L1 and L2 regularization to the network and train the MLP in this way. How does\n",
    "these regularizations affect the accuracy? This proportion can be varied as a tunable hyperparameter that can be\n",
    "explored as part of other project requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d06f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b18959f",
   "metadata": {},
   "source": [
    "## 4 - Unnormalized 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293319c9",
   "metadata": {},
   "source": [
    "Create an MLP with 2 hidden layers each having 256 units with ReLU activations as above. However, this time,\n",
    "train it with unnormalized images. How does this affect the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbda2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "390db30d",
   "metadata": {},
   "source": [
    "## 5 - Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0923a4",
   "metadata": {},
   "source": [
    "Re-train the MLP from question 3 on a version of FashionMNIST using data augmentation. You can use the\n",
    "transforms.Compose() function to set your transformations for data augmentation, and the\n",
    "transform=train transform argument in the dataset constructor to set the transforms. Is the accuracy\n",
    "affected, and how? What are the benefits and/or drawbacks of using data augmentation? Can you think of a\n",
    "situation in which certain types data augmentation would be harmful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc88a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data and add other transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d1113",
   "metadata": {},
   "source": [
    "# CNN Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1b6b2",
   "metadata": {},
   "source": [
    "## 6 - Create a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed89f06",
   "metadata": {},
   "source": [
    "Using existing libraries such as TensorFlow or PyTorch, create a convolutional neural network (CNN) with 2\n",
    "convolutional layers, one fully connected hidden layer and one fully connected output layer. Although you\n",
    "are free in your choice of the hyperparameters of the convolutional layers, set the number of units in the fully\n",
    "connected layers to be 256. Also, set the activations in all of the layers to be ReLU. Train this CNN on the\n",
    "FashionMNIST dataset. Does using a CNN increase/decrease the accuracy compared to using MLPs? Provide\n",
    "comments on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cad2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef926f7",
   "metadata": {},
   "source": [
    "## 7 - Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867f56b",
   "metadata": {},
   "source": [
    "Train the above CNN using FashionMNIST with the data augmentation from Q5. How is the performance\n",
    "(accuracy and speed) affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06887ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f13b9c",
   "metadata": {},
   "source": [
    "## 8 - Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bf3e2",
   "metadata": {},
   "source": [
    "Load a pre-trained model that you see fit (e.g., a ResNet) using existing libraries such as TensorFlow or PyTorch,\n",
    "and then freeze all the convolutional layers and remove all the fully connected ones. Add a number of fully\n",
    "connected layers of your choice right after the convolutional layers. Train only the fully connected layers of the\n",
    "pre-trained model on the FashionMNIST dataset with the data augmentation from Q5. How does this pre-trained\n",
    "model compare to the best MLP in part 5 and to the CNN in part 7 in terms of accuracy? How does it compare\n",
    "to the previous models in terms of the required training time? Justify your choice of how many fully connected\n",
    "layers that you have added to the pre-trained model through careful experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68daf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa65932",
   "metadata": {},
   "source": [
    "# TO CLEAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find better way to normalize dataset *****FIXED? SEE LOADING DATA SECTION\n",
    "\n",
    "def compute_loader_stats(loader, return_distribution=False):\n",
    "    n_pixels = 0\n",
    "    sum_pixels = 0\n",
    "    sum_squared = 0\n",
    "    \n",
    "    dist = []\n",
    "    for images, _ in loader:\n",
    "        n_pixels += images.numel()\n",
    "        sum_pixels += images.sum().item()\n",
    "        sum_squared += (images ** 2).sum().item()\n",
    "        if return_distribution:\n",
    "            dist.append(images.numpy().flatten())\n",
    "        \n",
    "    mean = sum_pixels / n_pixels\n",
    "    var = (sum_squared / n_pixels) - (mean ** 2)\n",
    "    std = np.sqrt(var)\n",
    "    return mean, std, dist if return_distribution else (mean, std)\n",
    "\n",
    "\n",
    "def compute_data_stats(*loaders, return_distribution=False):\n",
    "    total_mean = 0\n",
    "    total_std = 0\n",
    "    for loader in loaders:\n",
    "        stats = compute_loader_stats(loader, return_distribution=return_distribution)\n",
    "        total_mean += stats[0]\n",
    "        total_std += stats[1]\n",
    "        print(f'Loader mean: {stats[0]:.4f}, std: {stats[1]:.4f}')\n",
    "    n_loaders = len(loaders)\n",
    "    print(f'Overall mean: {total_mean / n_loaders:.4f}, std: {total_std / n_loaders:.4f}')\n",
    "    total_mean /= n_loaders\n",
    "    total_std /= n_loaders\n",
    "    return total_mean, total_std, stats[2] if return_distribution else (total_mean, total_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ddd6231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp variables until normalization done\n",
    "norm_train = train_dataset\n",
    "norm_test = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = compute_data_stats(train_loader, test_loader, return_distribution=True)\n",
    "# mean, std = stats[0], stats[1]\n",
    "# print(f'Final mean: {mean:.4f}, std: {std:.4f}')\n",
    "\n",
    "# distribution_prenorm = np.concatenate(stats[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88c8e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(distribution_prenorm, bins=100, kde=True)\n",
    "# plt.yscale('log')\n",
    "# plt.title('Pixel Value Distribution Before Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c4b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize datasets\n",
    "# train_dataset.transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((mean,), (std,))\n",
    "# ])\n",
    "\n",
    "# test_dataset.transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((mean,), (std,))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8fbcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = compute_data_stats(train_loader, test_loader, return_distribution=True)\n",
    "# mean_postnorm, std_postnorm = stats[0], stats[1]\n",
    "# print(f'Final mean: {mean_postnorm:.4f}, std: {std_postnorm:.4f}')\n",
    "\n",
    "# distribution_postnorm = np.concatenate(stats[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c5c80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(distribution_postnorm, bins=100, kde=True)\n",
    "# plt.yscale('log')\n",
    "# plt.title('Pixel Value Distribution Before Normalization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp551",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
