{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e62a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cf56f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "018ac04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "relu = lambda z: np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca989841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    "    )\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "# get loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find better way to normalize dataset\n",
    "\n",
    "def compute_loader_stats(loader, return_distribution=False):\n",
    "    n_pixels = 0\n",
    "    sum_pixels = 0\n",
    "    sum_squared = 0\n",
    "    \n",
    "    dist = []\n",
    "    for images, _ in loader:\n",
    "        n_pixels += images.numel()\n",
    "        sum_pixels += images.sum().item()\n",
    "        sum_squared += (images ** 2).sum().item()\n",
    "        if return_distribution:\n",
    "            dist.append(images.numpy().flatten())\n",
    "        \n",
    "    mean = sum_pixels / n_pixels\n",
    "    var = (sum_squared / n_pixels) - (mean ** 2)\n",
    "    std = np.sqrt(var)\n",
    "    return mean, std, dist if return_distribution else (mean, std)\n",
    "\n",
    "\n",
    "def compute_data_stats(*loaders, return_distribution=False):\n",
    "    total_mean = 0\n",
    "    total_std = 0\n",
    "    for loader in loaders:\n",
    "        stats = compute_loader_stats(loader, return_distribution=return_distribution)\n",
    "        total_mean += stats[0]\n",
    "        total_std += stats[1]\n",
    "        print(f'Loader mean: {stats[0]:.4f}, std: {stats[1]:.4f}')\n",
    "    n_loaders = len(loaders)\n",
    "    print(f'Overall mean: {total_mean / n_loaders:.4f}, std: {total_std / n_loaders:.4f}')\n",
    "    total_mean /= n_loaders\n",
    "    total_std /= n_loaders\n",
    "    return total_mean, total_std, stats[2] if return_distribution else (total_mean, total_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ddd6231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp variables until normalization done\n",
    "norm_train = train_dataset\n",
    "norm_test = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = compute_data_stats(train_loader, test_loader, return_distribution=True)\n",
    "# mean, std = stats[0], stats[1]\n",
    "# print(f'Final mean: {mean:.4f}, std: {std:.4f}')\n",
    "\n",
    "# distribution_prenorm = np.concatenate(stats[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88c8e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(distribution_prenorm, bins=100, kde=True)\n",
    "# plt.yscale('log')\n",
    "# plt.title('Pixel Value Distribution Before Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c4b324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize datasets\n",
    "# train_dataset.transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((mean,), (std,))\n",
    "# ])\n",
    "\n",
    "# test_dataset.transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((mean,), (std,))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8fbcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = compute_data_stats(train_loader, test_loader, return_distribution=True)\n",
    "# mean_postnorm, std_postnorm = stats[0], stats[1]\n",
    "# print(f'Final mean: {mean_postnorm:.4f}, std: {std_postnorm:.4f}')\n",
    "\n",
    "# distribution_postnorm = np.concatenate(stats[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c5c80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(distribution_postnorm, bins=100, kde=True)\n",
    "# plt.yscale('log')\n",
    "# plt.title('Pixel Value Distribution Before Normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66478ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Constructor takes:\n",
    "    g = output activation function\n",
    "    h = hidden activation function\n",
    "    L = number of hidden layers\n",
    "    M = number of hidden units, iterable (each element corresponds to a layer)\n",
    "    \n",
    "    -> Weights and biases are initialized randomly\n",
    "    \"\"\"\n",
    "    def __init__(self, g=relu, h=relu, L=1, M = 64): \n",
    "        self.g = g\n",
    "        self.h = h\n",
    "        self.L = L\n",
    "        \n",
    "        if isinstance(M, int):\n",
    "            self.M = M\n",
    "        else:\n",
    "            assert len(M) == L, \"Length of M must equal L\"\n",
    "            self.M = M\n",
    "        \n",
    "    def fit(self, x, y, optimizer):\n",
    "        N,D = x.shape\n",
    "        def gradient(x, y, params):\n",
    "            v, w = params\n",
    "            z = logistic(np.dot(x, v)) #N x M\n",
    "            yh = logistic(np.dot(z, w))#N\n",
    "            dy = yh - y #N\n",
    "            dw = np.dot(z.T, dy)/N #M\n",
    "            dz = np.outer(dy, w) #N x M\n",
    "            dv = np.dot(x.T, dz * z * (1 - z))/N #D x M\n",
    "            dparams = [dv, dw]\n",
    "            return dparams\n",
    "        \n",
    "        w = np.random.randn(self.M) * .01\n",
    "        v = np.random.randn(D,self.M) * .01\n",
    "        params0 = [v,w]\n",
    "        self.params = optimizer.run(gradient, x, y, params0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        v, w = self.params\n",
    "        z = logistic(np.dot(x, v)) #N x M\n",
    "        yh = logistic(np.dot(z, w))#N\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e002be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, params):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            grad = gradient_fn(x, y, params)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
